{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c54e4f0-79ce-4f6c-b6c8-7f1098e86c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock ETL Pipeline initialized in development environment\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, logging, requests, sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('/Users/justinclancy/Documents/Projects/Stock_ETL_Pipeline2/scripts/__init__.py'))))\n",
    "\n",
    "from config.settings import Settings\n",
    "Settings.api.ALPHA_VANTAGE_API_KEY = 'FT9DV93ELSDEH3FS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d504b2bb-6251-4828-b274-f86c7d6b1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for this module\n",
    "logging.basicConfig(\n",
    "    level = getattr(logging, Settings.LOG_LEVEL),\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers = [\n",
    "        logging.FileHandler(os.path.join(Settings.paths.LOGS_DIR, 'fetch_stock_data.log')),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54eb08c3-aecd-42a4-97f5-a760db1e4ad8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class StockDataFetcher:\n",
    "    \"\"\"\n",
    "    A robust stock data fetcher that handles API interactions with Alpha Vantage.\n",
    "\n",
    "    This class encapsulates all the complexity of working with external APIs:\n",
    "    - Authentication and API key management\n",
    "    - Rate limiting compliance\n",
    "    - Error handling and retreis\n",
    "    - Data validation and format conversion\n",
    "\n",
    "    Example usage:\n",
    "        fetcher = StockDataFetcher()\n",
    "        data    = fetcher.fetch_daily_data(['AAPL', 'GOOGL'])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the stock data fetcher.\n",
    "\n",
    "        Args:\n",
    "            api_key: ALpha Vantage API key. If None, uses config default.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or Settings.api.ALPHA_VANTAGE_API_KEY\n",
    "        self.base_url = Settings.api.ALPHA_VANTAGE_BASE_URL\n",
    "        self.session = requests.Session() # Reuse connections for efficiency\n",
    "\n",
    "        # Rate limiting state\n",
    "        self.last_request_time = 0\n",
    "        self.requests_made = 0\n",
    "        self.request_times = []\n",
    "\n",
    "        logger.info(f\"StockDataFetcher initialized with base URL: {self.base_url}\")\n",
    "\n",
    "    def _respect_rate_limit(self) -> None:\n",
    "        \"\"\"\n",
    "        Implement rate limiting to comply with API restrictions.\n",
    "\n",
    "        Alpha Vantage free tier allows 5 requests per minute.\n",
    "        This method ensures we don't exceed that limit.\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Remove request times older than 1 minute\n",
    "        one_minute_ago = current_time - 60\n",
    "        self.request_times = [t for t in self.request_times if t > one_minute_ago]\n",
    "\n",
    "        # If we've made too many requests in the last minute, wait\n",
    "        if len(self.request_times) >= Settings.api.REQUESTS_PER_MINUTE:\n",
    "            sleep_time = 60 - (current_time - self.request_times[0])\n",
    "            if sleep_time > 0:\n",
    "                logger.info(f\"Rate limit reached. Waiting {sleep_time:.1f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "        \n",
    "        # Record this request time\n",
    "        self.request_times.append(current_time)\n",
    "        self.last_request_time = current_time\n",
    "\n",
    "    def _make_api_request(self, params: Dict[str,str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Make a single API request with error handling and retries\n",
    "\n",
    "        Args:\n",
    "            params: Dictionary of API parameters\n",
    "\n",
    "        Returns:\n",
    "            JSON response as a dictionary\n",
    "        \n",
    "        Raises: \n",
    "            requests.RequestException: if all retry attempts fail\n",
    "        \"\"\"\n",
    "        # Add API key to parameters\n",
    "        params['apikey'] = self.api_key\n",
    "\n",
    "        for attempt in range(Settings.api.RETRY_ATTEMPTS):\n",
    "            try:\n",
    "                # Respect rate limiting\n",
    "                self._respect_rate_limit()\n",
    "\n",
    "                logger.debug(f\"Making API request (attempt {attempt + 1}): {params}\")\n",
    "\n",
    "                # Make the request\n",
    "                response = self.session.get(self.base_url, params = params, timeout = 30)\n",
    "                response.raise_for_status() # Raise an error for bad status codes\n",
    "\n",
    "                # Parse JSON response\n",
    "                data = response.json()\n",
    "\n",
    "                # Check for API -specific error messages\n",
    "                if 'Error Message' in data:\n",
    "                    raise ValueError(f\"API Error: {data['Error Message']}\")\n",
    "                \n",
    "                if 'Note' in data:\n",
    "                    logger.warning(f\"API Note: {data['None']}\")\n",
    "                    # This usually means rate limit hit, wait and retry\n",
    "                    time.sleep(Settings.api.RETRY_DELAY)\n",
    "                    continue\n",
    "\n",
    "                logger.debug(\"API request successful\")\n",
    "                return data\n",
    "            except (requests.RequestException, ValueError, KeyError) as e:\n",
    "                logger.warning(f\"API request failed (attempt {attempt + 1}): {str(e)}\")\n",
    "\n",
    "                if attempt < Settings.api.RETRY_ATTEMPTS - 1:\n",
    "                    wait_time = Settings.api.RETRY_DELAY * (2**attempt) # Exponential backoff\n",
    "                    logger.info(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logger.error(f\"All retry attempts failed for params: {params}\")\n",
    "                    raise\n",
    "        # This should never be reached, but just in case\n",
    "        raise requests.RequestException('Unexpected error in API request')\n",
    "    \n",
    "    def fetch_daily_data(self, symbol: str, outputsize: str = 'compact') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch daily stock data for a single symbol.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol (e.g., 'AAPL')\n",
    "            outputsize: 'compact' (100 days) or 'full' (all available)\n",
    "        \n",
    "        Returns:\n",
    "            pandas DataFrame with columns: date, open, high, low, close, volume    \n",
    "        \"\"\"\n",
    "        logger.info(f\"Fetching daily data for {symbol}\")\n",
    "\n",
    "        params = {\n",
    "            'function': 'TIME_SERIES_DAILY_ADJUSTED',\n",
    "            'symbol': symbol,\n",
    "            'outputsize': outputsize\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            data = self._make_api_request(params)\n",
    "\n",
    "            # Extract the time series data\n",
    "            time_series_key = 'Time Series (Daily)'\n",
    "            if time_series_key not in data:\n",
    "                raise KeyError(f\"Expected key '{time_series_key} not found in API response\")\n",
    "            \n",
    "            time_series = data[time_series_key]\n",
    "\n",
    "            # Convert to pandas DataFrame\n",
    "            df = pd.DataFrame.from_dict(time_series, orient = 'index')\n",
    "\n",
    "            # Clean column names and data types\n",
    "            df = self._clean_daily_data(df, symbol)\n",
    "\n",
    "            logger.info(f\"Successfully fetched {len(df)} days of data for {symbol}\")\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch data for {symbol}: {str(e)}\")\n",
    "            # Return empty DataFrame with correct columns for consistency\n",
    "            return pd.DataFrame(columns = ['symbol','data','open','high','low','close','adjusted_close','volume'])\n",
    "        \n",
    "    def _clean_daily_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and standardize the raw API data.\n",
    "\n",
    "        This method demonstrates important data cleaning patterns:\n",
    "        1. Column renaming and standardization\n",
    "        2. Data type conversion\n",
    "        3. Index handling\n",
    "        4. Data validation\n",
    "\n",
    "        Args:\n",
    "            df: Raw DataFrame from API\n",
    "            symbol: Stock symbol for metadata\n",
    "\n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # Rename columns to remove API previxes\n",
    "        column_mapping = {\n",
    "            '1. open': 'open',\n",
    "            '2. high': 'high',\n",
    "            '3. low': 'low',\n",
    "            '4. close': 'close',\n",
    "            '5. adjusted close': 'adjusted_close',\n",
    "            '6. volume': 'volume'\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "\n",
    "        # Convert index (dates) to datetime\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.index.name = 'date'\n",
    "\n",
    "        # Reset index to make date a column\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Add symbol column\n",
    "        df['symbol'] = symbol\n",
    "\n",
    "        # Convert price columns to numeric\n",
    "        price_columns = ['open', 'high', 'low', 'close', 'adjusted_close']\n",
    "        for col in price_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors = 'coerce')\n",
    "        \n",
    "        # Convert volume to integer\n",
    "        df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        # Sort by date (oldest first)\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        # Data validation\n",
    "        self._validate_price_data(df,symbol)\n",
    "\n",
    "        # Reorder columns for consistency\n",
    "        df = df[['symbol','date','open','high','low','close','adjusted_close','volume']]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def _validate_price_data(self, df: pd.DataFrame, symbol: str) -> None:\n",
    "        \"\"\"\n",
    "        Validate the cleaned price data for common issues.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "            symbol: Stock symbol for error messages\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            logger.warning(f\"No data available for {symbol}\")\n",
    "            return\n",
    "\n",
    "        # Check for missing values in critical columns\n",
    "        critical_columns = ['open','high','low','close']\n",
    "        missing_data = df[critical_columns].isnull().sum()\n",
    "        if missing_data.any():\n",
    "            logger.warning(f\"Missing data in {symbol}: {missing_data[missing_data>0].to_dict()}\")\n",
    "\n",
    "        # Check for impossible price relationships\n",
    "        invalid_ohlc = df[\n",
    "            (df['high'] < df['low']) | \n",
    "            (df['open'] < df['low']) | \n",
    "            (df['open'] > df['high']) | \n",
    "            (df['close'] < df['low']) | \n",
    "            (df['close'] > df['high'])\n",
    "        ]\n",
    "\n",
    "        if not invalid_ohlc.empty:\n",
    "            logger.warning(f\"Invalid OHLC relationships found for {symbol} on dates: {invalid_ohlc['date'].tolist()}\")\n",
    "\n",
    "        # Check for negative prices\n",
    "        negative_prices = df[(df[critical_columns] <= 0).any(axis=1)]\n",
    "        if not negative_prices.empty:\n",
    "            logger.warning(f\"Negative or zero prices found for {symbol} on dates: {negative_prices['date'].tolist()}\")\n",
    "        \n",
    "        logger.debug(f\"Data validation completed for {symbol}\")\n",
    "\n",
    "    def fetch_multiple_symbols(self, symbols: List[str], save_individual: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch data for multiple stock symbols.\n",
    "\n",
    "        This method demonstrates batch processing patterns and error handling\n",
    "        when working with multiple data sources.\n",
    "\n",
    "        Args:\n",
    "            symbols: List of stock symbols\n",
    "            save_individual: Whether to save each symbol's data separately\n",
    "\n",
    "        Returns:\n",
    "            Combined DataFrame with all symbols' data\n",
    "        \"\"\"\n",
    "        logger.info(f\"Fetching data for {len(symbols)} symbols: {symbols}\")\n",
    "\n",
    "        all_data = []\n",
    "        failed_symbols = []\n",
    "\n",
    "        for i, symbol in enumerate(symbols):\n",
    "            logger.info(f\"Processing {symbol} ({i+1}/{len(symbols)})\")\n",
    "\n",
    "            try:\n",
    "                df = self.fetch_daily_data(symbol)\n",
    "\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "\n",
    "                    # Save individual symbol data if requested\n",
    "                    if save_individual: \n",
    "                        self._save_symbol_data(df,symbol)\n",
    "                else:\n",
    "                    failed_symbols.append(symbol)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {symbol}: {str(e)}\")\n",
    "                failed_symbols.append(symbol)\n",
    "\n",
    "        if failed_symbols:\n",
    "            logger.warning(f\"Failed to fetch data for symbols: {failed_symbols}\")\n",
    "\n",
    "        # Combine all data\n",
    "        if all_data:\n",
    "            combined_df = pd.concat(all_data, ignore_index = True)\n",
    "            logger.info(f\"Successfully fetched data for {len(all_data)} symbols, total records: {len(combined_df)}\")\n",
    "\n",
    "        else:\n",
    "            logger.warning(\"No data was sucessfully fetched\")\n",
    "            combined_df = pd.DataFrame()\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def _save_symbol_data(self, df: pd.DataFrame, symbol: str) -> None:\n",
    "        \"\"\"\n",
    "        Save individual symbol data to files.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            symbol: Stock symbol\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        # Save as CSV\n",
    "        csv_filename = f\"{symbol}_daily_{timestamp}.csv\"\n",
    "        csv_path = os.path.join(Settings.paths.RAW_DATA_DIR, csv_filename)\n",
    "        df.to_csv(csv_path, index = False)\n",
    "\n",
    "        # Save as JSON for backup\n",
    "        json_filename = f\"{symbol}_daily_{timestamp}.json\"\n",
    "        json_path = os.path.join(Settings.paths.RAW_DATA_DIR, json_filename)\n",
    "        df.to_json(json_path, orient = 'records', date_format = 'iso', indent = 2)\n",
    "\n",
    "        logger.debug(f\"Saved {symbol} data to {csv_path} and {json_path}\")\n",
    "\n",
    "    def save_combined_data(self, df: pd.DataFrame, filename_prefix: str = 'stock_data') -> str:\n",
    "        \"\"\"\n",
    "        Save combined stock data with timestamp.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            filename_prefix: Prefix for the filename\n",
    "\n",
    "        Returns:\n",
    "            Path to saved file\n",
    "        \"\"\" \n",
    "        if df.empty:\n",
    "            logger.warning(\"No data to save\")\n",
    "            return \"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "        filepath = os.path.join(Settings.paths.RAW_DATA_DIR, filename)\n",
    "\n",
    "        # Save with metadata header\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(f\"# Stock data extracted at {datetime.now().isoformat()}\\n\")\n",
    "            f.write(f\"# Symbols: {sorted(df['symbol'].unique().tolist())}\\n\")\n",
    "            f.write(f\"# Records: {len(df)}\\n\")\n",
    "            f.write(f\"# Date range: {df['date'].min()} to {df['date'].max()}\\n\")\n",
    "            f.write(\"# Columns: symbol, date, open, high, low, close, adjusted_close, volume\\n\")\n",
    "\n",
    "        # Append the actual data\n",
    "        df.to_csv(filepath, mode = 'a', index = False)\n",
    "\n",
    "        logger.info(f\"Combined data saved to {filepath}\")\n",
    "        return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b0646c-2b52-4138-9136-7e101d192521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 11:49:32,446 - __main__ - INFO - Starting stock data exctraction process\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting stock data exctraction process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8d5516-205b-4277-a20b-1399239d52ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 11:49:32,451 - __main__ - INFO - StockDataFetcher initialized with base URL: https://www.alphavantage.co/query\n"
     ]
    }
   ],
   "source": [
    "fetcher = StockDataFetcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd54be7-48c4-4e90-b3c3-7fa9d14dc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 11:49:32,459 - __main__ - INFO - Will fetch data for symbols: ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX']\n"
     ]
    }
   ],
   "source": [
    "# Get symbols from configuration\n",
    "symbols = Settings.stock.DEFAULT_SYMBOLS\n",
    "logger.info(f\"Will fetch data for symbols: {symbols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db01411-b288-442a-b747-0feb40a75731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol = 'AAPL'\n",
    "# outputsize = 'compact'\n",
    "# params = {\n",
    "#     'function': 'TIME_SERIES_DAILY',\n",
    "#     'symbol': symbol,\n",
    "#     'outputsize': outputsize,\n",
    "#     'apikey': 'FT9DV93ELSDEH3FS'\n",
    "# }\n",
    "# # API key = FT9DV93ELSDEH3FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ef6130-fec0-4881-967e-086c814c9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=IBM&apikey=FT9DV93ELSDEH3FS'\n",
    "r = requests.get(url)\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5287dd9-19bb-481b-8fc6-aebb4ad3021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'function': 'TIME_SERIES_DAILY',\n",
    "            'symbol': 'AAPL',\n",
    "            'outputsize':'compact'\n",
    "        }\n",
    "data = fetcher._make_api_request(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ee0ee47-52a6-4dcc-976c-c3d75da681ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 11:51:18,816 - __main__ - INFO - Successfully fetched 100 days of data for AAPL\n"
     ]
    }
   ],
   "source": [
    "symbol = 'AAPL'\n",
    "time_series_key = 'Time Series (Daily)'\n",
    "if time_series_key not in data:\n",
    "    raise KeyError(f\"Expected key '{time_series_key} not found in API response\")\n",
    "time_series = data[time_series_key]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(time_series, orient = 'index')\n",
    "column_mapping = {\n",
    "    '1. open': 'open',\n",
    "    '2. high': 'high',\n",
    "    '3. low': 'low',\n",
    "    '4. close': 'close',\n",
    "    '5. volume': 'volume'\n",
    "}\n",
    "df = df.rename(columns=column_mapping)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.index.name = 'date'\n",
    "\n",
    "# Reset index to make date a column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Add symbol column\n",
    "df['symbol'] = symbol\n",
    "\n",
    "# Convert price columns to numeric\n",
    "price_columns = ['open', 'high', 'low', 'close']\n",
    "for col in price_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors = 'coerce')\n",
    "# Convert volume to integer\n",
    "df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Sort by date (oldest first)\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Data validation\n",
    "fetcher._validate_price_data(df,symbol)\n",
    "\n",
    "# Reorder columns for consistency\n",
    "df = df[['symbol','date','open','high','low','close','volume']]\n",
    "logger.info(f\"Successfully fetched {len(df)} days of data for {'AAPL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec966bd8-1c0d-4286-9b10-0219969e3eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 11:51:24,124 - __main__ - INFO - Fetching data for 8 symbols: ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX']\n",
      "2025-09-17 11:51:24,125 - __main__ - INFO - Processing AAPL (1/8)\n",
      "2025-09-17 11:51:24,127 - __main__ - INFO - Fetching daily data for AAPL\n",
      "2025-09-17 11:51:24,834 - __main__ - ERROR - Failed to fetch data for AAPL: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:51:24,838 - __main__ - INFO - Processing GOOGL (2/8)\n",
      "2025-09-17 11:51:24,840 - __main__ - INFO - Fetching daily data for GOOGL\n",
      "2025-09-17 11:51:25,087 - __main__ - ERROR - Failed to fetch data for GOOGL: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:51:25,092 - __main__ - INFO - Processing MSFT (3/8)\n",
      "2025-09-17 11:51:25,093 - __main__ - INFO - Fetching daily data for MSFT\n",
      "2025-09-17 11:51:25,327 - __main__ - ERROR - Failed to fetch data for MSFT: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:51:25,331 - __main__ - INFO - Processing AMZN (4/8)\n",
      "2025-09-17 11:51:25,333 - __main__ - INFO - Fetching daily data for AMZN\n",
      "2025-09-17 11:51:25,561 - __main__ - ERROR - Failed to fetch data for AMZN: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:51:25,564 - __main__ - INFO - Processing TSLA (5/8)\n",
      "2025-09-17 11:51:25,565 - __main__ - INFO - Fetching daily data for TSLA\n",
      "2025-09-17 11:51:25,804 - __main__ - ERROR - Failed to fetch data for TSLA: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:51:25,808 - __main__ - INFO - Processing META (6/8)\n",
      "2025-09-17 11:51:25,809 - __main__ - INFO - Fetching daily data for META\n",
      "2025-09-17 11:51:25,810 - __main__ - INFO - Rate limit reached. Waiting 58.3 seconds...\n",
      "2025-09-17 11:52:24,379 - __main__ - ERROR - Failed to fetch data for META: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:52:24,384 - __main__ - INFO - Processing NVDA (7/8)\n",
      "2025-09-17 11:52:24,386 - __main__ - INFO - Fetching daily data for NVDA\n",
      "2025-09-17 11:52:24,387 - __main__ - INFO - Rate limit reached. Waiting 0.5 seconds...\n",
      "2025-09-17 11:52:25,094 - __main__ - ERROR - Failed to fetch data for NVDA: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:52:25,098 - __main__ - INFO - Processing NFLX (8/8)\n",
      "2025-09-17 11:52:25,099 - __main__ - INFO - Fetching daily data for NFLX\n",
      "2025-09-17 11:52:25,329 - __main__ - ERROR - Failed to fetch data for NFLX: \"Expected key 'Time Series (Daily) not found in API response\"\n",
      "2025-09-17 11:52:25,333 - __main__ - WARNING - Failed to fetch data for symbols: ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX']\n",
      "2025-09-17 11:52:25,335 - __main__ - WARNING - No data was sucessfully fetched\n"
     ]
    }
   ],
   "source": [
    "combined_data = fetcher.fetch_multiple_symbols(symbols, save_individual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a484b-e30c-4df3-bae8-113aa7997e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
